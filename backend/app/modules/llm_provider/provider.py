"""
LLM Provider — Multi-backend LLM loading, prompt formatting and generation.

Supports four modes (configured via LLM_PROVIDER env var):
  - "gemini"       → Google Gemini API (recommended, uses API key)
  - "llama_cpp"    → llama-cpp-python with GGUF quantised models
  - "transformers" → HuggingFace transformers pipeline
  - "none"         → disabled; chat engine falls back to rule-based responses

For Gemini: set GEMINI_API_KEY in your .env file.
For Llama: auto-downloads the model on first run if LLAMA_MODEL_PATH is empty.
"""

import logging
import os
from pathlib import Path
from typing import Optional, List, Dict

from app.config.settings import settings

logger = logging.getLogger(__name__)

# ── Llama-2 / Llama-3 chat prompt templates ─────────────────────
SYSTEM_PROMPT = """You are an expert AI compliance assistant for AIComplianceGuard.
You are knowledgeable about ISO 27001, ISO 9001, NIST CSF, GDPR/PDPA, and the CIA triad.
You help users analyze compliance documents, find gaps, identify weak policies, and suggest improvements.
Answer concisely in well-structured Markdown. Use bullet points and headings for clarity.
If the user has uploaded a document, reference specific clauses when possible."""


def _build_llama2_prompt(system: str, messages: List[Dict[str, str]]) -> str:
    """Build a Llama-2-chat style prompt string."""
    parts = [f"<s>[INST] <<SYS>>\n{system}\n<</SYS>>\n\n"]
    for i, m in enumerate(messages):
        role, content = m["role"], m["content"]
        if role == "user":
            if i == 0:
                parts.append(f"{content} [/INST]")
            else:
                parts.append(f"<s>[INST] {content} [/INST]")
        elif role == "assistant":
            parts.append(f" {content} </s>")
    return "".join(parts)


def _build_llama3_prompt(system: str, messages: List[Dict[str, str]]) -> str:
    """Build a Llama-3-chat style prompt string."""
    parts = [
        "<|begin_of_text|>"
        f"<|start_header_id|>system<|end_header_id|>\n\n{system}<|eot_id|>"
    ]
    for m in messages:
        role = m["role"]
        parts.append(
            f"<|start_header_id|>{role}<|end_header_id|>\n\n{m['content']}<|eot_id|>"
        )
    parts.append("<|start_header_id|>assistant<|end_header_id|>\n\n")
    return "".join(parts)


class LLMProvider:
    """
    Unified LLM wrapper. Call `generate()` with a system prompt and
    a list of {role, content} messages.
    """

    def __init__(self):
        self.provider = settings.LLM_PROVIDER.lower()
        self._model = None
        self._tokenizer = None
        self._pipeline = None
        self._gemini_model = None
        self._is_loaded = False

        if self.provider == "none":
            logger.info("LLM provider disabled (LLM_PROVIDER=none). Using rule-based chat.")
            return

        logger.info(f"LLM provider: {self.provider}")

    # ── Lazy loading ──────────────────────────────────────────────
    @property
    def is_available(self) -> bool:
        if self.provider == "none":
            return False
        if not self._is_loaded:
            try:
                self._load_model()
            except Exception as e:
                logger.error(f"LLM model failed to load: {e}")
                return False
        return self._is_loaded

    def _resolve_model_path(self) -> str:
        """Return the local path to the GGUF file, downloading if needed."""
        if settings.LLAMA_MODEL_PATH and Path(settings.LLAMA_MODEL_PATH).exists():
            return settings.LLAMA_MODEL_PATH

        # Auto-download from Hugging Face Hub
        cache_dir = Path(settings.MODEL_CACHE_DIR)
        cache_dir.mkdir(parents=True, exist_ok=True)
        local_path = cache_dir / settings.LLAMA_MODEL_FILE

        if local_path.exists():
            return str(local_path)

        logger.info(
            f"Downloading Llama GGUF model: {settings.LLAMA_MODEL_REPO} / "
            f"{settings.LLAMA_MODEL_FILE} → {local_path}"
        )
        try:
            from huggingface_hub import hf_hub_download

            downloaded = hf_hub_download(
                repo_id=settings.LLAMA_MODEL_REPO,
                filename=settings.LLAMA_MODEL_FILE,
                local_dir=str(cache_dir),
                local_dir_use_symlinks=False,
            )
            logger.info(f"Model downloaded to: {downloaded}")
            return downloaded
        except Exception as e:
            logger.error(f"Failed to download model: {e}")
            raise

    # ── Model loaders ─────────────────────────────────────────────
    def _load_model(self):
        if self._is_loaded:
            return

        if self.provider == "gemini":
            self._load_gemini()
        elif self.provider == "llama_cpp":
            self._load_llama_cpp()
        elif self.provider == "transformers":
            self._load_transformers()
        else:
            logger.warning(f"Unknown LLM_PROVIDER: {self.provider}")

    def _load_gemini(self):
        """Load Google Gemini model via API key."""
        try:
            import google.generativeai as genai
        except ImportError:
            raise ImportError(
                "google-generativeai is not installed. "
                "Run:  pip install google-generativeai"
            )

        api_key = settings.GEMINI_API_KEY
        if not api_key:
            raise RuntimeError(
                "GEMINI_API_KEY is not set. Add it to your .env file."
            )

        genai.configure(api_key=api_key)

        generation_config = genai.types.GenerationConfig(
            max_output_tokens=settings.GEMINI_MAX_TOKENS,
            temperature=settings.GEMINI_TEMPERATURE,
            top_p=settings.GEMINI_TOP_P,
        )

        self._gemini_model = genai.GenerativeModel(
            model_name=settings.GEMINI_MODEL,
            generation_config=generation_config,
        )
        self._is_loaded = True
        logger.info(f"Gemini model loaded: {settings.GEMINI_MODEL}")

    def _load_llama_cpp(self):
        """Load model via llama-cpp-python."""
        try:
            from llama_cpp import Llama  # type: ignore
        except ImportError:
            raise ImportError(
                "llama-cpp-python is not installed. "
                "Run:  pip install llama-cpp-python  "
                "(add --extra-index-url for GPU builds)"
            )

        model_path = self._resolve_model_path()

        n_gpu = settings.LLAMA_N_GPU_LAYERS
        if settings.USE_GPU:
            n_gpu = max(n_gpu, 35)  # offload most layers

        logger.info(f"Loading Llama-cpp model: {model_path} (n_gpu_layers={n_gpu})")
        self._model = Llama(
            model_path=model_path,
            n_ctx=settings.LLAMA_CONTEXT_LENGTH,
            n_gpu_layers=n_gpu,
            n_threads=settings.LLAMA_N_THREADS,
            verbose=False,
        )
        self._is_loaded = True
        logger.info("Llama-cpp model loaded successfully")

    def _load_transformers(self):
        """Load model via HuggingFace transformers."""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
            import torch
        except ImportError:
            raise ImportError("transformers and torch are required for the 'transformers' provider.")

        model_id = settings.LLAMA_HF_MODEL
        logger.info(f"Loading HF model: {model_id}")

        dtype = torch.float16 if settings.USE_GPU else torch.float32
        device_map = "auto" if settings.USE_GPU else "cpu"

        self._tokenizer = AutoTokenizer.from_pretrained(
            model_id, cache_dir=settings.MODEL_CACHE_DIR
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            cache_dir=settings.MODEL_CACHE_DIR,
            torch_dtype=dtype,
            device_map=device_map,
        )
        self._pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=self._tokenizer,
        )
        self._is_loaded = True
        logger.info("HF transformers model loaded successfully")

    # ── Generation ────────────────────────────────────────────────
    def generate(
        self,
        messages: List[Dict[str, str]],
        system_prompt: str = SYSTEM_PROMPT,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        context_info: str = "",
    ) -> str:
        """
        Generate a response from the Llama model.

        Args:
            messages: conversation history [{role: 'user'|'assistant', content: '...'}]
            system_prompt: system instructions
            context_info: extra context injected into the system prompt
                          (e.g. document clauses, framework data)
            max_tokens: override settings.LLAMA_MAX_TOKENS
            temperature: override settings.LLAMA_TEMPERATURE

        Returns:
            Generated text string.
        """
        if not self.is_available:
            raise RuntimeError("LLM is not available. Check logs for loading errors.")

        full_system = system_prompt
        if context_info:
            full_system += f"\n\n### Context\n{context_info}"

        max_tok = max_tokens or settings.LLAMA_MAX_TOKENS
        temp = temperature or settings.LLAMA_TEMPERATURE

        if self.provider == "gemini":
            return self._generate_gemini(full_system, messages, max_tok, temp)
        elif self.provider == "llama_cpp":
            return self._generate_llama_cpp(full_system, messages, max_tok, temp)
        elif self.provider == "transformers":
            return self._generate_transformers(full_system, messages, max_tok, temp)
        else:
            raise RuntimeError(f"Unknown provider: {self.provider}")

    def _generate_gemini(
        self,
        system: str,
        messages: List[Dict[str, str]],
        max_tokens: int,
        temperature: float,
    ) -> str:
        """Generate response using Google Gemini API."""
        import google.generativeai as genai

        # Build Gemini-compatible chat history
        gemini_history = []
        for m in messages[:-1]:  # all except the last user message
            role = "user" if m["role"] == "user" else "model"
            gemini_history.append({"role": role, "parts": [m["content"]]})

        # Start chat with system prompt and history
        chat = self._gemini_model.start_chat(history=gemini_history)

        # The last message should be the user's current query
        last_message = messages[-1]["content"] if messages else ""

        # Prepend system context for the first message
        if system and not gemini_history:
            last_message = f"[System Instructions]: {system}\n\n[User Query]: {last_message}"

        response = chat.send_message(
            last_message,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=temperature,
                top_p=settings.GEMINI_TOP_P,
            ),
        )
        return response.text.strip()

    def _generate_llama_cpp(
        self,
        system: str,
        messages: List[Dict[str, str]],
        max_tokens: int,
        temperature: float,
    ) -> str:
        prompt = _build_llama2_prompt(system, messages)

        response = self._model(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=settings.LLAMA_TOP_P,
            stop=["</s>", "[INST]"],
            echo=False,
        )
        text = response["choices"][0]["text"].strip()
        return text

    def _generate_transformers(
        self,
        system: str,
        messages: List[Dict[str, str]],
        max_tokens: int,
        temperature: float,
    ) -> str:
        prompt = _build_llama2_prompt(system, messages)

        outputs = self._pipeline(
            prompt,
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=settings.LLAMA_TOP_P,
            do_sample=True,
            return_full_text=False,
        )
        text = outputs[0]["generated_text"].strip()
        return text


# ── Singleton ─────────────────────────────────────────────────────
llm_provider = LLMProvider()
